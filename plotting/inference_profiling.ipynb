{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/renbo/Desktop/New-Ops/neuraloperator-branches/neuraloperator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import os\n",
    "import time\n",
    "from configmypy import ConfigPipeline, YamlConfig, ArgparseConfig\n",
    "from neuralop import get_model\n",
    "from neuralop import Trainer\n",
    "from neuralop.training import setup\n",
    "from neuralop.datasets.navier_stokes import load_navier_stokes_pt\n",
    "from neuralop.utils import get_wandb_api_key, count_params, get_project_root, set_seed\n",
    "from neuralop import LpLoss, H1Loss\n",
    "from neuralop.models.spectral_convolution import FactorizedSpectralConv\n",
    "import torch.nn as nn\n",
    "from torch.cuda import amp \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting seed to 123\n",
      "UnitGaussianNormalizer init on 10000, reducing over [0, 1, 2, 3], samples of shape [1, 128, 128].\n",
      "   Mean and std of shape torch.Size([1, 1, 1]), eps=1e-05\n",
      "Given argument key='skip' that is not in TFNO2d's signature.\n",
      "Keyword argument non_linearity not specified for model TFNO2d, using default=<built-in function gelu>.\n",
      "Keyword argument fno_skip not specified for model TFNO2d, using default=linear.\n",
      "Keyword argument mlp_skip not specified for model TFNO2d, using default=soft-gating.\n",
      "Keyword argument decomposition_kwargs not specified for model TFNO2d, using default={}.\n"
     ]
    }
   ],
   "source": [
    "#from torch.ao.quantization import QConfigMapping\n",
    "#from torch.ao.quantization.qconfig_mapping import get_default_qconfig_mapping\n",
    "#from torch.ao.quantization.fx.custom_config import PrepareCustomConfig\n",
    "\n",
    "# Note that this is temporary, we'll expose these functions to torch.ao.quantization after official releasee\n",
    "#from torch.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "\n",
    "# ignore complexhalf warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def get_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    size = os.path.getsize(\"temp.p\")/1e6\n",
    "    os.remove('temp.p')\n",
    "    return size\n",
    "\n",
    "def replace_layers(model, old, new):\n",
    "    for n, module in model.named_children():\n",
    "        if len(list(module.children())) > 0:\n",
    "            ## compound module, go inside it\n",
    "            replace_layers(module, old, new)\n",
    "            \n",
    "        if isinstance(module, old):\n",
    "            ## simple module\n",
    "            #new = new.from_float(module)\n",
    "            setattr(model, n, new)\n",
    "\n",
    "\n",
    "# Read the configuration\n",
    "config_name = 'default'\n",
    "#config_folder = os.path.join(get_project_root(), 'config')\n",
    "config_folder = os.path.join('..', 'config')\n",
    "config_file_name = 'load_8layer_config.yaml'\n",
    "\n",
    "pipe = ConfigPipeline([YamlConfig(config_file_name, config_name=config_name, config_folder=config_folder),\n",
    "                       ArgparseConfig(infer_types=True, config_name=None, config_file=None),\n",
    "                       YamlConfig(config_folder=config_folder)\n",
    "                      ])\n",
    "config = pipe.read_conf()\n",
    "config_name = pipe.steps[-1].config_name\n",
    "\n",
    "# Set seed\n",
    "if 'seed' in config and config.seed:\n",
    "    print('setting seed to', config.seed)\n",
    "    set_seed(config.seed)\n",
    "\n",
    "#Set-up distributed communication, if using\n",
    "device, is_logger = setup(config)\n",
    "\n",
    "# Make sure we only print information when needed\n",
    "config.verbose = config.verbose and is_logger\n",
    "\n",
    "# Loading the Navier-Stokes dataset in 128x128 resolution\n",
    "train_loader, test_loaders, output_encoder = load_navier_stokes_pt(\n",
    "        config.data.folder, train_resolution=config.data.train_resolution, n_train=config.data.n_train, batch_size=config.data.batch_size, \n",
    "        positional_encoding=config.data.positional_encoding,\n",
    "        test_resolutions=config.data.test_resolutions, n_tests=config.data.n_tests, test_batch_sizes=config.data.test_batch_sizes,\n",
    "        encode_input=config.data.encode_input, encode_output=config.data.encode_output,\n",
    "        num_workers=config.data.num_workers, pin_memory=config.data.pin_memory, persistent_workers=config.data.persistent_workers\n",
    "        )\n",
    "model = get_model(config)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2loss = LpLoss(d=2, p=2)\n",
    "h1loss = H1Loss(d=2)\n",
    "eval_losses={'h1': h1loss, 'l2': l2loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on regular inputs (no multi-grid patching).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load model from checkpoint\n",
    "checkpoint_path = os.path.join('../checkpoints', 'full_precision.pt')\n",
    "trainer = Trainer(model, n_epochs=config.opt.n_epochs,\n",
    "                  device=device,\n",
    "                  mg_patching_levels=config.patching.levels,\n",
    "                  mg_patching_padding=config.patching.padding,\n",
    "                  mg_patching_stitching=config.patching.stitching,\n",
    "                  wandb_log=config.wandb.log,\n",
    "                  amp_autocast=config.opt.amp_autocast,\n",
    "                  precision_schedule=config.opt.precision_schedule,\n",
    "                  log_test_interval=config.wandb.log_test_interval,\n",
    "                  log_output=config.wandb.log_output,\n",
    "                  use_distributed=config.distributed.use_distributed,\n",
    "                  verbose=config.verbose and is_logger)\n",
    "\n",
    "#Create the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                                lr=config.opt.learning_rate, \n",
    "                                weight_decay=config.opt.weight_decay)\n",
    "\n",
    "# load model from dict\n",
    "model_load_epoch = -1\n",
    "trainer.load_model_checkpoint(model_load_epoch, model, optimizer, load_path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We try inference on full precision model and with added AMP. AMP makes inference slightly faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", 128_h1=0.0105, 128_l2=0.0041\n"
     ]
    }
   ],
   "source": [
    "for loader_name, loader in test_loaders.items():\n",
    "    to_log_output = True\n",
    "    msg = ''\n",
    "    errors = trainer.evaluate(model, eval_losses, loader, output_encoder, log_prefix=loader_name)\n",
    "\n",
    "    for loss_name, loss_value in errors.items():\n",
    "        msg += f', {loss_name}={loss_value:.4f}'\n",
    "\n",
    "    print(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", 128_h1=0.0112, 128_l2=0.0041\n"
     ]
    }
   ],
   "source": [
    "for loader_name, loader in test_loaders.items():\n",
    "    msg = ''\n",
    "    with amp.autocast(enabled=True):\n",
    "        errors = trainer.evaluate(model, eval_losses, loader, output_encoder, log_prefix=loader_name)\n",
    "\n",
    "    for loss_name, loss_value in errors.items():\n",
    "        msg += f', {loss_name}={loss_value:.4f}'\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, we are casting the model to half-precision and with added AMP, we perform inference. The results degrades significantly in half-precision when trained in full-precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", 128_h1=0.0690, 128_l2=0.0577\n"
     ]
    }
   ],
   "source": [
    "import copy \n",
    "model_fp16 = copy.deepcopy(model)\n",
    "model_fp16.fno_blocks.convs.half_prec_fourier = False\n",
    "model_fp16.fno_blocks.convs.half_prec_inverse = True\n",
    "\n",
    "for loader_name, loader in test_loaders.items():\n",
    "    msg = ''\n",
    "    with amp.autocast(enabled=True):\n",
    "        errors = trainer.evaluate(model_fp16, eval_losses, loader, output_encoder, log_prefix=loader_name)\n",
    "    for loss_name, loss_value in errors.items():\n",
    "        msg += f', {loss_name}={loss_value:.4f}'\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m loader_name, loader \u001b[39min\u001b[39;00m test_loaders\u001b[39m.\u001b[39mitems():\n\u001b[1;32m      3\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m     errors \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mevaluate(model_fp16, eval_losses, loader, output_encoder, log_prefix\u001b[39m=\u001b[39;49mloader_name)\n\u001b[1;32m      5\u001b[0m     \u001b[39mfor\u001b[39;00m loss_name, loss_value \u001b[39min\u001b[39;00m errors\u001b[39m.\u001b[39mitems():\n\u001b[1;32m      6\u001b[0m         msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mloss_name\u001b[39m}\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m{\u001b[39;00mloss_value\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/nas-fno/lib/python3.9/site-packages/neuralop/training/trainer.py:356\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, model, loss_dict, data_loader, output_encoder, log_prefix)\u001b[0m\n\u001b[1;32m    353\u001b[0m             wandb\u001b[39m.\u001b[39mlog({\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mimage_\u001b[39m\u001b[39m{\u001b[39;00mlog_prefix\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m: wandb\u001b[39m.\u001b[39mImage(img\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())}, commit\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    355\u001b[0m         \u001b[39mfor\u001b[39;00m loss_name, loss \u001b[39min\u001b[39;00m loss_dict\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 356\u001b[0m             errors[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mlog_prefix\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mloss_name\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss(out, y)\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m    358\u001b[0m \u001b[39mdel\u001b[39;00m x, y, out\n\u001b[1;32m    360\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m errors\u001b[39m.\u001b[39mkeys():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# try without amp\n",
    "for loader_name, loader in test_loaders.items():\n",
    "    msg = ''\n",
    "    errors = trainer.evaluate(model_fp16, eval_losses, loader, output_encoder, log_prefix=loader_name)\n",
    "    for loss_name, loss_value in errors.items():\n",
    "        msg += f', {loss_name}={loss_value:.4f}'\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('nas-fno')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32381ce65a9f12e97232a973358ac53743f155fe9c54b0bc347acb8d48d30651"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
